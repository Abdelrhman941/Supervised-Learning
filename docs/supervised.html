<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning - Ultimate Reference</title>
    <style>
        :root {
            --primary-color: #4a6fa5;
            --secondary-color: #166088;
            --accent-color: #4fc3f7;
            --text-color: #333;
            --bg-color: #f9f9f9;
            --card-bg: #fff;
            --border-color: #ddd;
            --code-bg: #f5f5f5;
            --table-header: #e9eef6;
            --shadow: 0 4px 6px rgba(0,0,0,0.1);
            --transition: all 0.3s ease;
        }

        .dark-theme {
            --primary-color: #64b5f6;
            --secondary-color: #4fc3f7;
            --accent-color: #81d4fa;
            --text-color: #e0e0e0;
            --bg-color: #121212;
            --card-bg: #1e1e1e;
            --border-color: #333;
            --code-bg: #2d2d2d;
            --table-header: #2c3e50;
            --shadow: 0 4px 6px rgba(0,0,0,0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            transition: var(--transition);
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        .sidebar {
            width: 280px;
            background-color: var(--card-bg);
            border-right: 1px solid var(--border-color);
            padding: 20px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            transition: var(--transition);
            z-index: 100;
        }

        .content {
            flex: 1;
            margin-left: 280px;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto 0 280px;
        }

        header {
            background-color: var(--primary-color);
            color: white;
            padding: 30px 20px;
            text-align: center;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        h1, h2, h3, h4 {
            margin-bottom: 15px;
            color: var(--primary-color);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
            margin-top: 40px;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 25px;
        }

        p {
            margin-bottom: 20px;
            font-size: 1.05rem;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.12);
        }

        pre {
            background-color: var(--code-bg);
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Consolas', 'Courier New', monospace;
        }

        code {
            font-family: 'Consolas', 'Courier New', monospace;
            background-color: var(--code-bg);
            padding: 2px 5px;
            border-radius: 3px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background-color: var(--table-header);
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: rgba(0,0,0,0.03);
        }

        tr:hover {
            background-color: rgba(0,0,0,0.05);
        }

        .btn {
            display: inline-block;
            background-color: var(--primary-color);
            color: white;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            text-decoration: none;
            font-size: 1rem;
            transition: var(--transition);
        }

        .btn:hover {
            background-color: var(--secondary-color);
            transform: translateY(-2px);
        }

        .theme-toggle {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .theme-toggle:hover {
            background-color: var(--secondary-color);
            transform: rotate(30deg);
        }

        .nav-links {
            list-style: none;
        }

        .nav-links li {
            margin-bottom: 10px;
        }

        .nav-links a {
            color: var(--text-color);
            text-decoration: none;
            display: block;
            padding: 8px 10px;
            border-radius: 4px;
            transition: var(--transition);
        }

        .nav-links a:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible {
            background-color: var(--card-bg);
            color: var(--text-color);
            cursor: pointer;
            padding: 18px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 1.1rem;
            border-radius: 8px;
            margin-bottom: 10px;
            box-shadow: var(--shadow);
            transition: var(--transition);
            position: relative;
        }

        .active, .collapsible:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible:after {
            content: '\002B';
            color: var(--text-color);
            font-weight: bold;
            float: right;
            margin-left: 5px;
        }

        .active:after {
            content: "\2212";
            color: white;
        }

        .collapsible-content {
            padding: 0 18px;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            background-color: var(--card-bg);
            border-radius: 0 0 8px 8px;
            margin-bottom: 20px;
        }

        .note {
            background-color: rgba(79, 195, 247, 0.1);
            border-left: 4px solid var(--accent-color);
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .warning {
            background-color: rgba(255, 152, 0, 0.1);
            border-left: 4px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .tip {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .comparison-table {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-card {
            flex: 1;
            min-width: 300px;
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 20px;
            box-shadow: var(--shadow);
        }

        .comparison-card h3 {
            color: var(--primary-color);
            margin-bottom: 15px;
            text-align: center;
        }

        .comparison-card ul {
            padding-left: 20px;
        }

        .comparison-card li {
            margin-bottom: 10px;
        }

        .math {
            font-style: italic;
            font-family: 'Times New Roman', Times, serif;
        }

        .image-container {
            text-align: center;
            margin: 20px 0;
        }

        .image-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        .caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9rem;
        }

        @media (max-width: 768px) {
            .sidebar {
                width: 0;
                padding: 0;
                overflow: hidden;
            }
            
            .content {
                margin-left: 0;
            }
            
            .sidebar.active {
                width: 250px;
                padding: 20px;
            }
            
            .menu-toggle {
                display: block;
                position: fixed;
                top: 20px;
                left: 20px;
                z-index: 1000;
                background-color: var(--primary-color);
                color: white;
                border: none;
                border-radius: 4px;
                padding: 10px;
                cursor: pointer;
            }
        }
    </style>
</head>
<body>
    <button class="theme-toggle" id="themeToggle">🌓</button>
    
    <div class="container">
        <aside class="sidebar">
            <h2>Contents</h2>
            <ul class="nav-links">
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#conceptual-explanation">Conceptual Explanation</a></li>
                <li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
                <li><a href="#practical-usage">Practical Usage</a></li>
                <li><a href="#algorithms">Key Algorithms</a></li>
                <li><a href="#comparisons">Comparisons & Best Practices</a></li>
                <li><a href="#evaluation">Evaluation Methods</a></li>
                <li><a href="#learning-aids">Learning Aids</a></li>
                <li><a href="#resources">Additional Resources</a></li>
            </ul>
        </aside>
        
        <main class="content">
            
            <section id="introduction" class="card">
                <h2>Introduction to Supervised Learning</h2>
                <p>Supervised learning is a paradigm in machine learning where an algorithm learns from labeled training data to make predictions or decisions. The term "supervised" refers to the presence of a "teacher" (the labeled data) that guides the learning process.</p>
                
                <div class="note">
                    <strong>Key Point:</strong> In supervised learning, the algorithm learns a mapping function from input variables (X) to an output variable (Y). The goal is to approximate this mapping function so well that when new input data is provided, the algorithm can predict the output variables for that data.
                </div>
            </section>
            
            <section id="conceptual-explanation" class="card">
                <h2>Conceptual Explanation</h2>
                
                <h3>What is Supervised Learning?</h3>
                <p>Supervised learning is a type of machine learning where the model is trained on labeled examples, meaning that each training example is paired with an output label. The model learns to predict the output from the input data. Once the model is trained to a sufficient level of performance, it can be used to predict the output for new, unseen inputs.</p>
                
                <button class="collapsible">Types of Supervised Learning Problems</button>
                <div class="collapsible-content">
                    <p>Supervised learning problems can be broadly categorized into two types:</p>
                    
                    <h4>1. Classification</h4>
                    <p>In classification problems, the output variable is a category or a class. The model learns to assign input data to one of the predefined categories. Examples include:</p>
                    <ul>
                        <li>Email spam detection (spam or not spam)</li>
                        <li>Image recognition (cat, dog, etc.)</li>
                        <li>Medical diagnosis (disease present or absent)</li>
                    </ul>
                    
                    <h4>2. Regression</h4>
                    <p>In regression problems, the output variable is a continuous value. The model learns to predict a numerical value based on input data. Examples include:</p>
                    <ul>
                        <li>House price prediction</li>
                        <li>Temperature forecasting</li>
                        <li>Stock price prediction</li>
                    </ul>
                </div>
                
                <h3>Key Components of Supervised Learning</h3>
                <p>The supervised learning process involves several key components:</p>
                
                <ul>
                    <li><strong>Training Data:</strong> Labeled examples used to train the model</li>
                    <li><strong>Features:</strong> Input variables or attributes</li>
                    <li><strong>Target Variable:</strong> Output variable to be predicted</li>
                    <li><strong>Hypothesis/Model:</strong> The function that maps inputs to outputs</li>
                    <li><strong>Loss Function:</strong> Measures how well the model is performing</li>
                    <li><strong>Optimization Algorithm:</strong> Method to minimize the loss function</li>
                </ul>
            </section>
            
            <section id="mathematical-foundation" class="card">
                <h2>Mathematical Foundation</h2>
                
                <p>Supervised learning can be formalized mathematically as follows:</p>
                
                <p>Given a set of N training examples of the form <span class="math">{(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}</span> where each <span class="math">xᵢ</span> is an input vector and <span class="math">yᵢ</span> is the corresponding output, the goal is to learn a function <span class="math">h: X → Y</span> such that <span class="math">h(x)</span> is a good predictor for the corresponding value of <span class="math">y</span>.</p>
                
                <h3>Loss Functions</h3>
                <p>Loss functions quantify how well a model is performing. Common loss functions include:</p>
                
                <ul>
                    <li><strong>Mean Squared Error (Regression):</strong> <span class="math">MSE = (1/n) Σ(yᵢ - ŷᵢ)²</span></li>
                    <li><strong>Cross-Entropy Loss (Classification):</strong> <span class="math">L = -Σ yᵢ log(ŷᵢ)</span></li>
                </ul>
                
                <div class="tip">
                    <strong>Intuition:</strong> The loss function acts as a compass, guiding the model toward better predictions by penalizing errors. The optimization algorithm then adjusts the model parameters to minimize this loss.
                </div>
            </section>
            
            <section id="practical-usage" class="card">
                <h2>Practical Usage</h2>
                
                <h3>Real-World Applications</h3>
                <p>Supervised learning is used in a wide range of applications across various industries:</p>
                
                <ul>
                    <li><strong>Finance:</strong> Credit scoring, fraud detection, stock price prediction</li>
                    <li><strong>Healthcare:</strong> Disease diagnosis, patient outcome prediction, medical image analysis</li>
                    <li><strong>Retail:</strong> Customer segmentation, recommendation systems, demand forecasting</li>
                    <li><strong>Manufacturing:</strong> Quality control, predictive maintenance, yield optimization</li>
                    <li><strong>Transportation:</strong> Traffic prediction, route optimization, autonomous vehicles</li>
                </ul>
                
                <h3>Code Example: Linear Regression</h3>
                <p>Here's a simple example of implementing linear regression using scikit-learn:</p>
                
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Generate sample data
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1) * 2

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Coefficient: {model.coef_[0][0]:.2f}")
print(f"Intercept: {model.intercept_[0]:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Plot the results
plt.scatter(X_test, y_test, color='blue', label='Actual data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Example')
plt.legend()
plt.show()</code></pre>
                
                <h3>Code Example: Classification with Random Forest</h3>
                <p>Here's an example of implementing a classification model using Random Forest:</p>
                
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Generate a random classification dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, 
                           n_redundant=5, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plot feature importances
feature_importances = model.feature_importances_
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), feature_importances[indices])
plt.xticks(range(X.shape[1]), indices)
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.show()</code></pre>
            </section>
            
            <section id="algorithms" class="card">
                <h2>Key Supervised Learning Algorithms</h2>
                
                <button class="collapsible">Linear Models</button>
                <div class="collapsible-content">
                    <h3>Linear Regression</h3>
                    <p>Linear regression models the relationship between a dependent variable and one or more independent variables using a linear equation.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Simple and interpretable</li>
                        <li>Works well when the relationship between variables is linear</li>
                        <li>Prone to underfitting complex relationships</li>
                    </ul>
                    
                    <h3>Logistic Regression</h3>
                    <p>Logistic regression is used for binary classification problems. It uses a logistic function to model the probability of a certain class.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Outputs probabilities between 0 and 1</li>
                        <li>Simple and interpretable</li>
                        <li>Works well for linearly separable classes</li>
                    </ul>
                </div>
                
                <button class="collapsible">Tree-Based Models</button>
                <div class="collapsible-content">
                    <h3>Decision Trees</h3>
                    <p>Decision trees use a tree-like model of decisions. They split the data into branches based on feature values to make predictions.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Highly interpretable</li>
                        <li>Can handle both numerical and categorical data</li>
                        <li>Prone to overfitting</li>
                    </ul>
                    
                    <h3>Random Forest</h3>
                    <p>Random Forest is an ensemble method that builds multiple decision trees and merges their predictions.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Reduces overfitting compared to individual decision trees</li>
                        <li>Handles high-dimensional data well</li>
                        <li>Less interpretable than a single decision tree</li>
                    </ul>
                    
                    <h3>Gradient Boosting</h3>
                    <p>Gradient Boosting builds trees sequentially, with each tree correcting the errors of the previous ones.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Often achieves state-of-the-art performance</li>
                        <li>Requires careful tuning of hyperparameters</li>
                        <li>Can be computationally intensive</li>
                    </ul>
                </div>
                
                <button class="collapsible">Support Vector Machines</button>
                <div class="collapsible-content">
                    <h3>Support Vector Machines (SVM)</h3>
                    <p>SVM finds the hyperplane that best separates classes in the feature space, maximizing the margin between the classes.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Effective in high-dimensional spaces</li>
                        <li>Works well when classes are separable</li>
                        <li>Can use different kernel functions for non-linear boundaries</li>
                        <li>Memory-intensive for large datasets</li>
                    </ul>
                </div>
                
                <button class="collapsible">Neural Networks</button>
                <div class="collapsible-content">
                    <h3>Neural Networks</h3>
                    <p>Neural networks are composed of layers of interconnected nodes (neurons) that process information. They can learn complex patterns in data.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Can model highly complex relationships</li>
                        <li>Require large amounts of data for optimal performance</li>
                        <li>Computationally intensive to train</li>
                        <li>Less interpretable than other models</li>
                    </ul>
                </div>
                
                <button class="collapsible">Instance-Based Learning</button>
                <div class="collapsible-content">
                    <h3>k-Nearest Neighbors (k-NN)</h3>
                    <p>k-NN classifies a data point based on the majority class of its k nearest neighbors in the feature space.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Simple and intuitive</li>
                        <li>No explicit training phase</li>
                        <li>Computationally intensive during prediction</li>
                        <li>Sensitive to the scale of features</li>
                    </ul>
                </div>
                
                <button class="collapsible">Probabilistic Models</button>
                <div class="collapsible-content">
                    <h3>Naive Bayes</h3>
                    <p>Naive Bayes is a probabilistic classifier based on Bayes' theorem with an assumption of independence between features.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Fast and efficient</li>
                        <li>Works well with high-dimensional data</li>
                        <li>Requires relatively little training data</li>
                        <li>Assumption of feature independence may not hold in practice</li>
                    </ul>
                </div>
            </section>
            
            <section id="comparisons" class="card">
                <h2>Comparisons & Best Practices</h2>
                
                <h3>Algorithm Comparison</h3>
                
                <div class="comparison-table">
                    <div class="comparison-card">
                        <h3>Linear Models</h3>
                        <h4>Strengths:</h4>
                        <ul>
                            <li>Simple and interpretable</li>
                            <li>Computationally efficient</li>
                            <li>Work well with linear relationships</li>
                            <li>Less prone to overfitting</li>
                        </ul>
                        <h4>Weaknesses:</h4>
                        <ul>
                            <li>Cannot capture non-linear relationships</li>
                            <li>Sensitive to outliers</li>
                            <li>Assume independence of features</li>
                        </ul>
                        <h4>Best for:</h4>
                        <ul>
                            <li>Simple, interpretable models</li>
                            <li>When relationships are approximately linear</li>
                            <li>When computational resources are limited</li>
                        </ul>
                    </div>
                    
                    <div class="comparison-card">
                        <h3>Tree-Based Models</h3>
                        <h4>Strengths:</h4>
                        <ul>
                            <li>Can capture non-linear relationships</li>
                            <li>Handle both numerical and categorical features</li>
                            <li>Robust to outliers</li>
                            <li>Feature importance readily available</li>
                        </ul>
                        <h4>Weaknesses:</h4>
                        <ul>
                            <li>Can overfit (especially single trees)</li>
                            <li>May not generalize well to unseen data</li>
                            <li>Ensemble methods can be computationally intensive</li>
                        </ul>
                        <h4>Best for:</h4>
                        <ul>
                            <li>Complex, non-linear relationships</li>
                            <li>When feature importance is needed</li>
                            <li>When dealing with mixed data types</li>
                        </ul>
                    </div>
                    
                    <div class="comparison-card">
                        <h3>Support Vector Machines</h3>
                        <h4>Strengths:</h4>
                        <ul>
                            <li>Effective in high-dimensional spaces</li>
                            <li>Versatile through different kernel functions</li>
                            <li>Memory efficient (only support vectors matter)</li>
                        </ul>
                        <h4>Weaknesses:</h4>
                        <ul>
                            <li>Difficult to interpret</li>
                            <li>Sensitive to choice of kernel and parameters</li>
                            <li>Slow training with large datasets</li>
                        </ul>
                        <h4>Best for:</h4>
                        <ul>
                            <li>Text classification and image recognition</li>
                            <li>When data has clear margins of separation</li>
                            <li>Medium-sized datasets with many features</li>
                        </ul>
                    </div>
                </div>
                
                <h3>When to Use What?</h3>
                <p>Choosing the right algorithm depends on several factors:</p>
                
                <table>
                    <tr>
                        <th>Factor</th>
                        <th>Recommended Algorithms</th>
                    </tr>
                    <tr>
                        <td>Small dataset</td>
                        <td>Linear models, Naive Bayes, SVM</td>
                    </tr>
                    <tr>
                        <td>Large dataset</td>
                        <td>Random Forest, Gradient Boosting, Neural Networks</td>
                    </tr>
                    <tr>
                        <td>High interpretability needed</td>
                        <td>Linear models, Decision Trees</td>
                    </tr>
                    <tr>
                        <td>Complex, non-linear relationships</td>
                        <td>Random Forest, Gradient Boosting, Neural Networks, SVM with non-linear kernels</td>
                    </tr>
                    <tr>
                        <td>High-dimensional data</td>
                        <td>SVM, Random Forest, Gradient Boosting</td>
                    </tr>
                    <tr>
                        <td>Imbalanced classes</td>
                        <td>Random Forest, Gradient Boosting (with appropriate weighting)</td>
                    </tr>
                    <tr>
                        <td>Fast prediction time needed</td>
                        <td>Linear models, Decision Trees, pre-trained Neural Networks</td>
                    </tr>
                </table>
                
                <div class="tip">
                    <strong>Best Practice:</strong> Always start with simple models (like linear models) as a baseline before moving to more complex ones. This helps in understanding the data and provides a benchmark for comparison.
                </div>
            </section>
            
            <section id="evaluation" class="card">
                <h2>Evaluation Methods</h2>
                
                <h3>Classification Metrics</h3>
                <ul>
                    <li><strong>Accuracy:</strong> Proportion of correct predictions (TP + TN) / (TP + TN + FP + FN)</li>
                    <li><strong>Precision:</strong> Proportion of positive identifications that were actually correct TP / (TP + FP)</li>
                    <li><strong>Recall (Sensitivity):</strong> Proportion of actual positives that were identified correctly TP / (TP + FN)</li>
                    <li><strong>F1 Score:</strong> Harmonic mean of precision and recall 2 * (Precision * Recall) / (Precision + Recall)</li>
                    <li><strong>ROC-AUC:</strong> Area under the Receiver Operating Characteristic curve, measures the model's ability to distinguish between classes</li>
                </ul>
                
                <h3>Regression Metrics</h3>
                <ul>
                    <li><strong>Mean Absolute Error (MAE):</strong> Average of absolute differences between predicted and actual values</li>
                    <li><strong>Mean Squared Error (MSE):</strong> Average of squared differences between predicted and actual values</li>
                    <li><strong>Root Mean Squared Error (RMSE):</strong> Square root of MSE</li>
                    <li><strong>R² Score:</strong> Proportion of variance in the dependent variable that is predictable from the independent variables</li>
                </ul>
                
                <h3>Cross-Validation</h3>
                <p>Cross-validation is a technique to evaluate how well a model will generalize to an independent dataset. Common methods include:</p>
                <ul>
                    <li><strong>k-Fold Cross-Validation:</strong> Data is divided into k subsets, and the model is trained and evaluated k times, each time using a different subset as the test set</li>
                    <li><strong>Leave-One-Out Cross-Validation:</strong> A special case of k-fold where k equals the number of data points</li>
                    <li><strong>Stratified k-Fold:</strong> Ensures that each fold has the same proportion of observations with a given label</li>
                </ul>
                
                <div class="note">
                    <strong>Important:</strong> Always evaluate your model on data it hasn't seen during training to get an accurate estimate of its performance on new data.
                </div>
            </section>
            
            <section id="learning-aids" class="card">
                <h2>Learning Aids</h2>
                
                <h3>Common Mistakes to Avoid</h3>
                <div class="warning">
                    <ul>
                        <li><strong>Data Leakage:</strong> Allowing information from the test set to influence the training process</li>
                        <li><strong>Overfitting:</strong> Creating a model that performs well on training data but poorly on new data</li>
                        <li><strong>Underfitting:</strong> Creating a model that is too simple to capture the underlying patterns in the data</li>
                        <li><strong>Ignoring Feature Scaling:</strong> Not normalizing or standardizing features when using algorithms sensitive to scale (e.g., SVM, k-NN)</li>
                        <li><strong>Using Accuracy for Imbalanced Classes:</strong> Relying solely on accuracy when classes are imbalanced</li>
                        <li><strong>Not Handling Missing Values:</strong> Failing to properly address missing data in the dataset</li>
                    </ul>
                </div>
                
                <h3>Best Practices Checklist</h3>
                <ul>
                    <li>✅ Understand your data through exploratory data analysis</li>
                    <li>✅ Split your data into training, validation, and test sets</li>
                    <li>✅ Address missing values and outliers</li>
                    <li>✅ Scale features when necessary</li>
                    <li>✅ Start with simple models and gradually increase complexity</li>
                    <li>✅ Use cross-validation to tune hyperparameters</li>
                    <li>✅ Evaluate models using appropriate metrics</li>
                    <li>✅ Consider the trade-off between model complexity and interpretability</li>
                    <li>✅ Document your process and findings</li>
                </ul>
                
                <h3>Workflow Diagram</h3>
                <div class="image-container">
                    <svg width="700" height="400" xmlns="http://www.w3.org/2000/svg">
                        <!-- Data Collection Box -->
                        <rect x="50" y="50" width="120" height="60" rx="10" ry="10" fill="#4a6fa5" />
                        <text x="110" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Data Collection</text>
                        
                        <!-- Data Preprocessing Box -->
                        <rect x="50" y="170" width="120" height="60" rx="10" ry="10" fill="#4a6fa5" />
                        <text x="110" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Data Preprocessing</text>
                        
                        <!-- Feature Engineering Box -->
                        <rect x="50" y="290" width="120" height="60" rx="10" ry="10" fill="#4a6fa5" />
                        <text x="110" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Feature Engineering</text>
                        
                        <!-- Model Selection Box -->
                        <rect x="230" y="50" width="120" height="60" rx="10" ry="10" fill="#166088" />
                        <text x="290" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Selection</text>
                        
                        <!-- Training Box -->
                        <rect x="230" y="170" width="120" height="60" rx="10" ry="10" fill="#166088" />
                        <text x="290" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Training</text>
                        
                        <!-- Hyperparameter Tuning Box -->
                        <rect x="230" y="290" width="120" height="60" rx="10" ry="10" fill="#166088" />
                        <text x="290" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Hyperparameter Tuning</text>
                        
                        <!-- Evaluation Box -->
                        <rect x="410" y="50" width="120" height="60" rx="10" ry="10" fill="#4fc3f7" />
                        <text x="470" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Evaluation</text>
                        
                        <!-- Interpretation Box -->
                        <rect x="410" y="170" width="120" height="60" rx="10" ry="10" fill="#4fc3f7" />
                        <text x="470" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Interpretation</text>
                        
                        <!-- Deployment Box -->
                        <rect x="410" y="290" width="120" height="60" rx="10" ry="10" fill="#4fc3f7" />
                        <text x="470" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Deployment</text>
                        
                        <!-- Monitoring Box -->
                        <rect x="590" y="170" width="120" height="60" rx="10" ry="10" fill="#333" />
                        <text x="650" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Monitoring & Maintenance</text>
                        
                        <!-- Arrows -->
                        <!-- Data Collection to Preprocessing -->
                        <line x1="110" y1="110" x2="110" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="110,170 105,160 115,160" fill="#333" />
                        
                        <!-- Preprocessing to Feature Engineering -->
                        <line x1="110" y1="230" x2="110" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="110,290 105,280 115,280" fill="#333" />
                        
                        <!-- Feature Engineering to Model Selection -->
                        <line x1="170" y1="320" x2="230" y2="80" stroke="#333" stroke-width="2" />
                        <polygon points="230,80 220,85 225,75" fill="#333" />
                        
                        <!-- Model Selection to Training -->
                        <line x1="290" y1="110" x2="290" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="290,170 285,160 295,160" fill="#333" />
                        
                        <!-- Training to Hyperparameter Tuning -->
                        <line x1="290" y1="230" x2="290" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="290,290 285,280 295,280" fill="#333" />
                        
                        <!-- Hyperparameter Tuning to Evaluation -->
                        <line x1="350" y1="320" x2="410" y2="80" stroke="#333" stroke-width="2" />
                        <polygon points="410,80 400,85 405,75" fill="#333" />
                        
                        <!-- Evaluation to Interpretation -->
                        <line x1="470" y1="110" x2="470" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="470,170 465,160 475,160" fill="#333" />
                        
                        <!-- Interpretation to Deployment -->
                        <line x1="470" y1="230" x2="470" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="470,290 465,280 475,280" fill="#333" />
                        
                        <!-- Deployment to Monitoring -->
                        <line x1="530" y1="320" x2="590" y2="200" stroke="#333" stroke-width="2" />
                        <polygon points="590,200 580,205 585,195" fill="#333" />
                        
                        <!-- Feedback loop from Monitoring to Data Collection -->
                        <path d="M 590,190 C 500,100 200,20 110,50" stroke="#333" stroke-width="2" fill="transparent" />
                        <polygon points="110,50 115,60 105,60" fill="#333" />
                    </svg>
                    <p class="caption">Supervised Learning Workflow</p>
                </div>
            </section>
            
            <section id="resources" class="card">
                <h2>Additional Resources</h2>
                
                <h3>Books</h3>
                <ul>
                    <li>"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurélien Géron</li>
                    <li>"An Introduction to Statistical Learning" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani</li>
                    <li>"Pattern Recognition and Machine Learning" by Christopher Bishop</li>
                </ul>
                
                <h3>Online Courses</h3>
                <ul>
                    <li>Andrew Ng's Machine Learning Course on Coursera</li>
                    <li>Fast.ai Practical Deep Learning for Coders</li>
                    <li>DataCamp's Supervised Learning with scikit-learn</li>
                </ul>
                
                <h3>Useful Libraries</h3>
                <ul>
                    <li>scikit-learn: Comprehensive machine learning library</li>
                    <li>TensorFlow/Keras: Deep learning frameworks</li>
                    <li>PyTorch: Deep learning framework with dynamic computation graphs</li>
                    <li>XGBoost: Optimized gradient boosting library</li>
                </ul>
            </section>
        </main>
    </div>
    
    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        themeToggle.addEventListener('click', () => {
            document.body.classList.toggle('dark-theme');
            localStorage.setItem('theme', document.body.classList.contains('dark-theme') ? 'dark' : 'light');
        });
        
        // Check for saved theme preference
        if (localStorage.getItem('theme') === 'dark') {
            document.body.classList.add('dark-theme');
        }
        
        // Collapsible sections
        const collapsibles = document.getElementsByClassName('collapsible');
        for (let i = 0; i < collapsibles.length; i++) {
            collapsibles[i].addEventListener('click', function() {
                this.classList.toggle('active');
                const content = this.nextElementSibling;
                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                } else {
                    content.style.maxHeight = content.scrollHeight + 'px';
                }
            });
        }
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>